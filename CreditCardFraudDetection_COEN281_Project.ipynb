{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfk0CobNvm0u"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, plot_roc_curve\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/sample_data/creditcard.csv')\n",
        "\n",
        "print(f\"Dataset shape:- \\n{data.shape}\")\n",
        "print(f\"Data features :- \\n{data.columns}\")\n"
      ],
      "metadata": {
        "id": "ZNP5aY8sw3Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalance in data"
      ],
      "metadata": {
        "id": "zKW8DP_Kw7WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "fraud = data[data['Class'] == 1]\n",
        "valid = data[data['Class'] == 0]\n",
        "print('Fraud Transactions: {}'.format(len(fraud)))\n",
        "print('Non-fraud Transactions: {}'.format(len(valid)))\n"
      ],
      "metadata": {
        "id": "arygmu_iw5b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing irrelevant columns"
      ],
      "metadata": {
        "id": "4rJgzHu1xp_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = data.drop(['Time'], axis = 1)\n",
        "print(f\"List of feature names after removing Time column:- \\n{data.columns}\")\n"
      ],
      "metadata": {
        "id": "t74scJ7zxGnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for null / Nan values"
      ],
      "metadata": {
        "id": "gtDkV5JrxwIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Dataset info:-\")\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "id": "qPnDA2wCxJ2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data transformation using Standard Scaler"
      ],
      "metadata": {
        "id": "Sa4AWFdNxyb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Few values of Amount column before Scaling :- \\n{data['Amount'][0:4]}\")\n",
        "data['Norm_Amount'] = StandardScaler().fit_transform(\n",
        "data['Amount'].values.reshape(-1,1))\n",
        "data = data.drop(['Amount'], axis = 1)\n",
        "print(f\"Few values of Amount column after applying StandardScaler:- \\n{data['Norm_Amount'][0:4]}\")\n"
      ],
      "metadata": {
        "id": "ytX88LiHxRT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate the performance metrics"
      ],
      "metadata": {
        "id": "99zXSnrKx4OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def performanceMetrics(y_test,y_predict):\n",
        "  # confusion matrix\n",
        "  LABELS = ['Normal', 'Fraud']\n",
        "  conf_matrix = confusion_matrix(y_test, y_predict)\n",
        "  sns.heatmap(conf_matrix, xticklabels = LABELS, yticklabels = LABELS, annot = True, fmt = 'd')\n",
        "  plt.title('Confusion matrix')\n",
        "  plt.xlabel('Predicted class')\n",
        "  plt.ylabel('True class')\n",
        "  plt.show()\n",
        " \n",
        "  # classification report \n",
        "  print(f\"Classification Report :- \\n\")\n",
        "  acc = accuracy_score(y_test, y_predict)\n",
        "  prec = precision_score(y_test, y_predict)\n",
        "  rec = recall_score(y_test, y_predict)\n",
        "  f1 = f1_score(y_test, y_predict)\n",
        "  print('accuracy: %0.4f'%acc,'\\tprecision: %0.4f'%prec,'\\trecall: %0.4f'%rec,'\\tF1-score: %0.4f'%f1)\n",
        " \n",
        "  # area under roc curve\n",
        "  print(f\"AROC score :- \\n {roc_auc_score(y_test, y_predict)}\")\n"
      ],
      "metadata": {
        "id": "KO4RjGUHxUMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Test Models"
      ],
      "metadata": {
        "id": "ol-2sQ-Xx7KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def trainAndTestModels():\n",
        "  print('\\n======== RandomForest ==========')\n",
        "  # initialize object for RandomForestClassifier class\n",
        "  rf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "  # train the classifier\n",
        "  rf_classifier.fit(X_train, y_train) \n",
        "  # predict result using test dataset\n",
        "  y_pred1 = rf_classifier.predict(X_test)\n",
        "  # measure performance\n",
        "  performanceMetrics(y_test,y_pred1)\n",
        "\n",
        "  print('\\n======== KNN ==========')  \n",
        "  # initialize object for KNeighborsClassifier class\n",
        "  knn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "  # train the classifier\n",
        "  knn_classifier.fit(X_train, y_train)\n",
        "  # predict result using test dataset\n",
        "  y_pred2 = knn_classifier.predict(X_test)\n",
        "  # measure performance\n",
        "  performanceMetrics(y_test,y_pred2)\n",
        "\n",
        "\n",
        "  print('\\n======== ANN ==========')\n",
        "  mlp_classifier = MLPClassifier(hidden_layer_sizes=(29,29,29), activation='relu', solver='adam', max_iter=500)\n",
        "  mlp_classifier.fit(X_train,y_train)\n",
        "  y_pred3 = mlp_classifier.predict(X_test)\n",
        "  # measure performance\n",
        "  performanceMetrics(y_test,y_pred3)\n",
        "\n",
        "\n",
        "  print('\\n======== Majority Voting ==========')\n",
        "  estimator = []\n",
        "  estimator.append(('RF', rf_classifier))\n",
        "  estimator.append(('KNN', knn_classifier))\n",
        "  estimator.append(('ANN', mlp_classifier))\n",
        "  \n",
        "  # Voting Classifier with hard voting\n",
        "  vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\n",
        "  vot_hard.fit(X_train, y_train)\n",
        "  y_pred = vot_hard.predict(X_test)\n",
        "\n",
        "  # measure performance\n",
        "  performanceMetrics(y_test,y_pred)"
      ],
      "metadata": {
        "id": "LuPF3TbXxYvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly Detection using OneClassSVM"
      ],
      "metadata": {
        "id": "qzxR_Pg6x9rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test = train_test_split(data, test_size=0.2, random_state=66)\n",
        "#Training with the observations having label 0 i.e , non-fraud transactions\n",
        "X_train = X_train[X_train.Class == 0]\n",
        "y_train = X_train.Class\n",
        "X_train = X_train.drop(['Class'], axis=1)\n",
        "y_test = X_test.Class\n",
        "X_test = X_test.drop(['Class'], axis=1)\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "\n",
        "\n",
        "# Training and testing the model\n",
        "model = OneClassSVM(gamma='auto', nu=0.05)\n",
        "model.fit(X_train) #Not all train data is used because of long training time\n",
        "y_pred = model.predict(X_test)\n",
        "# y_pred = y_pred.apply(lambda x: 1 if x == -1 else 0)\n",
        "for i in range(len(y_pred)):\n",
        "    y_pred[i] = 1 if y_pred[i] == -1 else 0\n",
        "\n",
        "performanceMetrics(y_test, y_pred)"
      ],
      "metadata": {
        "id": "lolVISXjxbiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without sampling"
      ],
      "metadata": {
        "id": "J7Ax3kEcyCnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = data.iloc[:,data.columns != 'Class']\n",
        "y = data.iloc[:,data.columns == 'Class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "trainAndTestModels()"
      ],
      "metadata": {
        "id": "moGxX91Dxc5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Undersampling "
      ],
      "metadata": {
        "id": "LDHf1-wxyFDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "fraud_indices = np.array(data[data.Class == 1].index)\n",
        "fraudTransactions = len(fraud_indices)\n",
        "nonFraud_indices = data[data.Class == 0].index\n",
        "# Random select N indices from non fraudulent samples (N equals to number of fraudulent records)\n",
        "random_normal_indices = np.random.choice(nonFraud_indices, fraudTransactions, replace=False)\n",
        "random_normal_indices = np.array(random_normal_indices)\n",
        "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
        "under_sample_data = data.iloc[under_sample_indices,:]\n",
        "X_undersample = under_sample_data.iloc[:,under_sample_data.columns != 'Class']\n",
        "y_undersample = under_sample_data.iloc[:,under_sample_data.columns == 'Class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_undersample,y_undersample, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "trainAndTestModels()"
      ],
      "metadata": {
        "id": "eUxHwaFvxgyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oversampling using SMOTE"
      ],
      "metadata": {
        "id": "-Rz84yBbyHf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "X_resample, y_resample = SMOTE().fit_resample(X,y.values.ravel())\n",
        "print('Number of total transactions before SMOTE sampling: ', len(y), '...after SMOTE upsampling: ', len(y_resample))\n",
        "print('Number of fraudulent transactions before SMOTE sampling: ', len(y[y.Class==1]), \n",
        "      '...after SMOTE upsampling: ', np.sum(y_resample[y_resample==1]))\n",
        "y_resample = pd.DataFrame(y_resample)\n",
        "X_resample = pd.DataFrame(X_resample)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resample,y_resample,test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "trainAndTestModels()"
      ],
      "metadata": {
        "id": "yAqr5O2pwtO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}